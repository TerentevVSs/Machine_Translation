{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Translation_Project_simple_LSTM_en_ru.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"i2Ko1khAESrf"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torchtext\n","from torchtext.legacy.data import Field, BucketIterator\n","\n","import spacy\n","from tqdm.notebook import tqdm\n","import tqdm\n","import random\n","import math\n","import time\n","import numpy as np\n","\n","import matplotlib\n","matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from IPython.display import clear_output\n","\n","from nltk.tokenize import WordPunctTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yeyNXU5GQ-LJ"},"source":["SEED = 666\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvr0cndHF7od","executionInfo":{"status":"ok","timestamp":1626304251473,"user_tz":-180,"elapsed":315,"user":{"displayName":"Владимир Сергеевич Терентьев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7ufn-QwNFAAI6ynabmbw-GLfoGiMOZvBGKv74Kg=s64","userId":"18403725243042882268"}},"outputId":"b4b56623-bc16-4042-eb16-658d106c4980"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZ1SYZfUPMF7","executionInfo":{"status":"ok","timestamp":1626304265839,"user_tz":-180,"elapsed":14368,"user":{"displayName":"Владимир Сергеевич Терентьев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7ufn-QwNFAAI6ynabmbw-GLfoGiMOZvBGKv74Kg=s64","userId":"18403725243042882268"}},"outputId":"cf6a998c-f4de-410c-f285-b11bae1bcc45"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DE-0MBIZRbLB","executionInfo":{"status":"ok","timestamp":1626304286630,"user_tz":-180,"elapsed":17,"user":{"displayName":"Владимир Сергеевич Терентьев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7ufn-QwNFAAI6ynabmbw-GLfoGiMOZvBGKv74Kg=s64","userId":"18403725243042882268"}},"outputId":"6bd32e2f-204f-488a-93ad-d0b141cbd37c"},"source":["!ls '/content/drive/MyDrive/Colab Notebooks/NLP'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["best_model.pt  europarl-v7.de-en.de  GRU_model2.pt\t new_data.txt\n","de-en.tgz      europarl-v7.de-en.en  GRU_model.pt\t rus-eng.zip\n","de-en.zip      fra-eng.zip\t     new_data_small.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jDbsLi5YJq9P"},"source":["tokenizer_W = WordPunctTokenizer()\n","\n","def tokenize_ru(x, tokenizer=WordPunctTokenizer()):\n","    return tokenizer.tokenize(x.lower())\n","\n","def tokenize_en(x, tokenizer=WordPunctTokenizer()):\n","    return tokenizer.tokenize(x.lower())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxjRcQAbJwwI"},"source":["SRC = Field(tokenize=tokenize_en,\n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True)\n","\n","TRG = Field(tokenize=tokenize_ru,\n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True)\n","\n","\n","dataset = torchtext.legacy.data.TabularDataset(\n","    path='data.txt',\n","    format='tsv',\n","    fields=[('src', SRC), ('trg', TRG)]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SHliJlHxJw9g","executionInfo":{"status":"ok","timestamp":1626304355403,"user_tz":-180,"elapsed":22,"user":{"displayName":"Владимир Сергеевич Терентьев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7ufn-QwNFAAI6ynabmbw-GLfoGiMOZvBGKv74Kg=s64","userId":"18403725243042882268"}},"outputId":"a0537a1c-81bf-459e-b7b4-2c3124bc9aeb"},"source":["print(len(dataset.examples))\n","print(dataset.examples[0].src)\n","print(dataset.examples[0].trg)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["291329\n","['wiederaufnahme', 'der', 'sitzungsperiode']\n","['resumption', 'of', 'the', 'session']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITHVChNxLAe-","executionInfo":{"status":"ok","timestamp":1626304355752,"user_tz":-180,"elapsed":360,"user":{"displayName":"Владимир Сергеевич Терентьев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7ufn-QwNFAAI6ynabmbw-GLfoGiMOZvBGKv74Kg=s64","userId":"18403725243042882268"}},"outputId":"c9e42b13-8a95-421d-cf57-e2e1a464037c"},"source":["train_data, valid_data, test_data = dataset.split(split_ratio=[0.8, 0.15, 0.05])\n","\n","print(f\"Number of training examples: {len(train_data.examples)}\")\n","print(f\"Number of validation examples: {len(valid_data.examples)}\")\n","print(f\"Number of testing examples: {len(test_data.examples)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training examples: 233063\n","Number of validation examples: 14567\n","Number of testing examples: 43699\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nFgcavqGLHT-"},"source":["SRC.build_vocab(dataset, min_freq=2)\n","TRG.build_vocab(dataset, min_freq=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mgPWlOaHLI_v","executionInfo":{"status":"ok","timestamp":1626304360693,"user_tz":-180,"elapsed":30,"user":{"displayName":"Владимир Сергеевич Терентьев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7ufn-QwNFAAI6ynabmbw-GLfoGiMOZvBGKv74Kg=s64","userId":"18403725243042882268"}},"outputId":"f24982f3-ecf2-4e52-f0ee-7710206ea98d"},"source":["print(f\"Unique tokens in source (en) vocabulary: {len(SRC.vocab)}\")\n","print(f\"Unique tokens in target (ru) vocabulary: {len(TRG.vocab)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Unique tokens in source (de) vocabulary: 22712\n","Unique tokens in target (en) vocabulary: 13824\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dq_JbomULJMv","executionInfo":{"status":"ok","timestamp":1626304360693,"user_tz":-180,"elapsed":23,"user":{"displayName":"Владимир Сергеевич Терентьев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7ufn-QwNFAAI6ynabmbw-GLfoGiMOZvBGKv74Kg=s64","userId":"18403725243042882268"}},"outputId":"ac82aa4f-2c75-4957-d855-b339fd43f5f8"},"source":["print(vars(train_data.examples[9]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'trg': ['in', 'particular', ',', 'if', 'flax', 'and', 'hemp', 'are', 'now', 'to', 'be', 'integrated', 'into', 'the', 'arable', 'support', 'system', ',', 'iacs', 'will', 'then', 'be', 'fully', 'applied', '.'], 'src': ['wenn', 'es', 'jetzt', 'vor', 'allem', 'bei', 'flachs', 'und', 'hanf', 'zu', 'einer', 'integration', 'in', 'das', 'ackerflächensystem', 'kommt', ',', 'wird', 'in', 'diesem', 'fall', 'das', 'invekos', '-', 'system', 'in', 'vollem', 'umfang', 'angewendet', '.']}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"goIMDbAZLNlv","executionInfo":{"status":"ok","timestamp":1626304360694,"user_tz":-180,"elapsed":14,"user":{"displayName":"Владимир Сергеевич Терентьев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7ufn-QwNFAAI6ynabmbw-GLfoGiMOZvBGKv74Kg=s64","userId":"18403725243042882268"}},"outputId":"b5f5ca40-7513-47c1-cba4-a9dbce94707d"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"Peyv-N1dLNaO"},"source":["def _len_sort_key(x):\n","    return len(x.src)\n","\n","BATCH_SIZE = 64\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE, \n","    device = device,\n","    sort_key=_len_sort_key\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gX_gSKhKUVqS"},"source":["class Encoder_simple(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout_prob, bidirectional=False):\n","        super().__init__()\n","\n","        self.input_dim = input_dim\n","        self.emb_dim = emb_dim\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","        self.dropout_prob = dropout_prob\n","        self.bidirectional = bidirectional\n","\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, \n","                           dropout=dropout_prob, bidirectional=bidirectional)\n","\n","        self.dropout = nn.Dropout(p=dropout_prob)\n","        \n","        \n","    def forward(self, src):\n","\n","        #src = [src len, batch size]\n","        embedded = self.dropout(self.embedding(src))\n","        #embedded = [src len, batch size, emb dim]\n","        \n","        output, (hidden, cell) = self.rnn(embedded)\n","        #output = [src len, batch size, hid dim * num directions]\n","        #hidden = [n layers * num directions, batch size, hid dim]\n","        #cell = [n layers, batch size, hid dim]\n","\n","        if self.bidirectional:\n","\n","            hidden = hidden.reshape(self.n_layers, 2, -1, self.hid_dim)\n","            hidden = hidden.transpose(1, 2).reshape(self.n_layers, -1, 2 * self.hid_dim)\n","\n","            cell = cell.reshape(self.n_layers, 2, -1, self.hid_dim)\n","            cell = cell.transpose(1, 2).reshape(self.n_layers, -1, 2 * self.hid_dim)\n","\n","        return hidden, cell"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DBHkDZZTUZ7Z"},"source":["class Decoder_simple(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout_prob):\n","        super().__init__()\n","\n","        self.emb_dim = emb_dim\n","        self.hid_dim = hid_dim\n","        self.output_dim = output_dim\n","        self.n_layers = n_layers\n","        self.dropout_prob = dropout_prob\n","\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, \n","                           dropout=dropout_prob)\n","        \n","        self.out = nn.Linear(hid_dim, output_dim)\n","\n","        self.dropout = nn.Dropout(p=dropout_prob)\n","\n","    def forward(self, input, hidden, cell):\n","\n","        #input = [batch size]\n","        #hidden = [batch size, dec hid dim]\n","        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n","\n","        unsqueeze_input = input.unsqueeze(0)\n","        #input = [1, batch size]\n","        \n","        embedded = self.dropout(self.embedding(unsqueeze_input))\n","        #embedded = [1, batch size, emb dim]\n","\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","        #output = [1, batch size, hid dim * num directions]\n","        #hidden = [n layers * num directions, batch size, hid dim]\n","        #cell = [n layers, batch size, hid dim]\n","\n","        prediction = self.out(output.squeeze(0))     \n","         \n","        return prediction, hidden, cell"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Me5Cl1xUdHC"},"source":["class Translator_simple(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self, src, trg, teacher_forcing):\n","\n","        #src = [src len, batch size]\n","        #trg = [trg len, batch size]\n","        \n","        batch_size = src.shape[1]\n","        trg_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","        \n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        hidden, cell = self.encoder(src)\n","\n","        input = trg[0,:]\n","\n","        for token in range(1, trg_len):\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","            outputs[token] = output\n","            teacher_force = random.random() < teacher_forcing\n","            top = output.argmax(-1) \n","            input = trg[token] if teacher_force else top\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"19JNxuiGhMY0"},"source":["input_dim = len(SRC.vocab)\n","output_dim = len(TRG.vocab)\n","encoder_embedding_dim = 256\n","decoder_embedding_dim = 256\n","hidden_layers = 512\n","layers = 2\n","encoder_dropout_prob = 0.5\n","decoder_dropout_prob = 0.5\n","bidirectional = True\n","\n","encoder = Encoder_simple(input_dim, encoder_embedding_dim, hidden_layers//2, layers, encoder_dropout_prob, bidirectional=bidirectional)\n","decoder = Decoder_simple(output_dim, decoder_embedding_dim, hidden_layers, layers, decoder_dropout_prob)\n","\n","model = Translator_simple(encoder, decoder, device).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aHmfUCzZw_GF","executionInfo":{"status":"ok","timestamp":1626305073752,"user_tz":-180,"elapsed":8,"user":{"displayName":"Владимир Сергеевич Терентьев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7ufn-QwNFAAI6ynabmbw-GLfoGiMOZvBGKv74Kg=s64","userId":"18403725243042882268"}},"outputId":"511edee6-47a8-4155-d4b5-bda81eb2ee6c"},"source":["print(input_dim)\n","print(output_dim)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["22712\n","13824\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7UwiVraiNNk","executionInfo":{"status":"ok","timestamp":1626305074556,"user_tz":-180,"elapsed":477,"user":{"displayName":"Владимир Сергеевич Терентьев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7ufn-QwNFAAI6ynabmbw-GLfoGiMOZvBGKv74Kg=s64","userId":"18403725243042882268"}},"outputId":"b9f23931-4d5a-48bd-9813-a7d44394388a"},"source":["def init_weights(m):\n","    # такая инициализация должна давать лучший результат \n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param, -0.08, 0.08)\n","        \n","model.apply(init_weights)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Translator_simple(\n","  (encoder): Encoder_simple(\n","    (embedding): Embedding(22712, 256)\n","    (rnn): LSTM(256, 256, num_layers=2, dropout=0.5, bidirectional=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder_simple(\n","    (embedding): Embedding(13824, 256)\n","    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n","    (out): Linear(in_features=512, out_features=13824, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PfByyvkqiawi","executionInfo":{"status":"ok","timestamp":1626305075311,"user_tz":-180,"elapsed":17,"user":{"displayName":"Владимир Сергеевич Терентьев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7ufn-QwNFAAI6ynabmbw-GLfoGiMOZvBGKv74Kg=s64","userId":"18403725243042882268"}},"outputId":"45a71875-64fc-436f-9a2d-8c5e7ccada7d"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 22,752,768 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4aANUWHwOyoz"},"source":["def delete_eos(tokens_iter):\n","    for token in tokens_iter:\n","        if token == '<eos>':\n","            break\n","        yield token\n","\n","def remove_tech_tokens(tokens_iter, tokens_to_remove=['<sos>', '<unk>', '<pad>']):\n","    return [x for x in tokens_iter if x not in tokens_to_remove]\n","\n","def generate_translation(src, trg, model, TRG_vocab):\n","    model.eval()\n","    # запускаем без teacher_forcing\n","    output = model(src, trg, 0)\n","    # удаляем первый токен и выбираем лучшее слово\n","    output = output[1:].argmax(-1)\n","    #print(output)\n","    original = remove_tech_tokens(delete_eos([TRG_vocab.itos[x] for x in list(trg[:,0].cpu().numpy())]))\n","    generated = remove_tech_tokens(delete_eos([TRG_vocab.itos[x] for x in list(output[:, 0].cpu().numpy())]))\n","    \n","    print('Правильный перевод: {}'.format(' '.join(original)))\n","    print('Перевод модели: {}'.format(' '.join(generated)))\n","\n","def get_text(x, TRG_vocab):\n","     generated = remove_tech_tokens(delete_eos([TRG_vocab.itos[elem] for elem in list(x)]))\n","     return generated\n","\n","from nltk.translate.bleu_score import corpus_bleu\n","\n","def get_bleu(iterator):\n","    original_text = []\n","    generated_text = []\n","    model.eval()\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            src = batch.src\n","            trg = batch.trg\n","            # запускаем без teacher_forcing\n","            output = model(src, trg, 0)\n","            # удаляем первый токен и выбираем лучшее слово\n","            output = output[1:].argmax(-1)\n","            # собираем данные для подсчета BLEU\n","            original_text.extend([get_text(x, TRG.vocab) for x in trg.cpu().numpy().T])\n","            generated_text.extend([get_text(x, TRG.vocab) for x in output.detach().cpu().numpy().T])\n","    bleu = corpus_bleu([[text] for text in original_text], generated_text) * 100\n","    return bleu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eXC-nwGtO0xM"},"source":["optimizer = optim.Adam(model.parameters())\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l3y5j2Gbic6z"},"source":["def train(model, iterator, optimizer, criterion, clip, epoch):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        \n","        src = batch.src\n","        trg = batch.trg\n","        \n","        optimizer.zero_grad()\n","        teacher_forcing = 1 - epoch * 0.25\n","        if teacher_forcing < 0.6:\n","            teacher_forcing = 0.6\n","        output = model(src, trg, teacher_forcing = teacher_forcing)\n","        \n","        #trg = [trg len, batch size]\n","        #output = [trg len, batch size, output dim]\n","        \n","        output_dim = output.shape[-1]\n","        \n","        output = output[1:].view(-1, output_dim)\n","        trg = trg[1:].view(-1)\n","        \n","        #trg = [(trg len - 1) * batch size]\n","        #output = [(trg len - 1) * batch size, output dim]\n","        \n","        loss = criterion(output, trg)\n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)\n","\n","def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(iterator):\n","\n","            src = batch.src\n","            trg = batch.trg\n","\n","            output = model(src, trg, 0) #turn off teacher forcing\n","\n","            #trg = [trg len, batch size]\n","            #output = [trg len, batch size, output dim]\n","\n","            output_dim = output.shape[-1]\n","            \n","            output = output[1:].view(-1, output_dim)\n","            trg = trg[1:].view(-1)\n","\n","            #trg = [(trg len - 1) * batch size]\n","            #output = [(trg len - 1) * batch size, output dim]\n","\n","            loss = criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvNqvLfJk69T"},"source":["epochs = 10\n","clip = 1\n","\n","best_valid_loss = float('inf')\n","best_valid_bleu = 0\n","for epoch in tqdm.notebook.tqdm(range(epochs)):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iterator, optimizer, criterion, clip, epoch)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","    valid_bleu = get_bleu(test_iterator)\n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/NLP/simple_LSTM_en_ru.pt')\n","    if valid_bleu > best_valid_bleu:\n","        best_valid_bleu= valid_bleu\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/NLP/simple_LSTM_bleu_en_ru.pt')\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n","    print(f'\\t Val. BLEU: {valid_bleu:.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LyCppsGkUQxn"},"source":["torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/NLP/simle_LSTM_last_en_ru.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oE0NQgv6Qjt7"},"source":["model_name = 'simple_LSTM_bleu_en_ru.pt'\n","model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/NLP/' + model_name))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h2rOWSlPPzIb"},"source":["def translate_batch(iterator):\n","    batch = next(iter(iterator))\n","    for idx in range(10):\n","        src = batch.src[:, idx:idx+1]\n","        trg = batch.trg[:, idx:idx+1]\n","        generate_translation(src, trg, model, TRG.vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O8R-V5PLQne8"},"source":["translate_batch(test_iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NkN2qB6OP41F"},"source":["print(\"Train BLEU = \", get_bleu(train_iterator))\n","print(\"Valid BLEU = \", get_bleu(valid_iterator))\n","print(\"Test BLEU = \", get_bleu(test_iterator))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lsyvwQ7u09-0"},"source":["def translate(data):\n","    with open('example.txt', 'w') as file:\n","        file.write(str(data*2+','+data))\n","    test_dataset = torchtext.legacy.data.TabularDataset(\n","        path='example.txt',\n","        format='csv',\n","        fields=[('src', SRC), ('trg', TRG)]\n","    )\n","    iterator = BucketIterator(\n","        test_dataset, \n","        batch_size = 1, \n","        device = device,\n","        sort_key=_len_sort_key\n","    )\n","    generated_text = []\n","    model.eval()\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            src = batch.src\n","            trg = batch.trg\n","            # запускаем без teacher_forcing\n","            output = model(src, trg, 0)\n","            # удаляем первый токен и выбираем лучшее слово\n","            output = output[1:].argmax(-1)\n","            # собираем данные для подсчета BLEU\n","            generated_text.extend([get_text(x, TRG.vocab) for x in output.detach().cpu().numpy().T])\n","            generated_text=(' '.join(generated_text[0])[:-2]+'.').capitalize()\n","    translation = 'Перевод модели: {}'.format(generated_text)\n","    return translation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"V5VA_n7h8vkM","outputId":"4f1768ca-3a94-4908-d17d-7e366e9debb4"},"source":["data='На всей территории гостевого дома Jam работает бесплатный Wi-Fi.'\n","translate(data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Перевод модели: Features free wifi throughout the property.'"]},"metadata":{"tags":[]},"execution_count":169}]}]}